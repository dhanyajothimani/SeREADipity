# -*- coding: utf-8 -*-
"""01_synopsis_similarity.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1th4G5IViw2YckD_1anZ1qoRKTGMhIKao

Estimate the synopsis similarity between book using BERT
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x

!pip install bert-serving-client

!pip install -U bert-serving-server[http]

!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip

!unzip uncased_L-12_H-768_A-12.zip

!nohup bert-serving-start -model_dir=./uncased_L-12_H-768_A-12 out.file2>&1&

from bert_serving.client import BertClient
from sklearn.metrics.pairwise import cosine_similarity
import nltk
nltk.download('punkt')

get_ipython().system_raw(
'bert-serving-start -model_dir=./uncased_L-12_H-768_A-12 -http_port 3333 &'
)

client = BertClient()

from google.colab import files
uploaded = files.upload() #File to be selected: book_data_bk.csv

import pandas as pd 
books_db = pd.read_csv(r"book_data_bk.csv",encoding = "ISO-8859-1", engine='python')
print(books_db.head())
print(books_db.shape)
book_db1 = books_db.drop_duplicates(subset='book_title', keep = 'first', inplace = False)
print(book_db1.shape)
book_db = book_db1.dropna()
print(book_db.shape)


print(book_db.columns)

book_subset = book_db[['book_title','book_authors','book_desc']]
print(book_subset.shape)

book_subset.isnull().values.any()

book_subset_NA = book_subset.dropna()
book_subset_NA.isnull().values.any()
book_subset_NA = book_subset_NA.reset_index()

def synopsis_similarity(df, sentences):

  df1 = pd.DataFrame()
  df1["book_title"] = ""
  df1["book_comp"] = ""
  df1["synopsis_sim"] = ""
          
  columns = list(df1.columns)

  sim_list = []
  for i in df.index:
    for j in df.index:
      if i != j:
        first_column = df.at[i, "book_title"]
        second_column = df.at[j, "book_title"]
        cos_sim = cosine_similarity(sentences[i,:].reshape(1,-1), sentences[j,:].reshape(1,-1))
        cos_sim1 = cos_sim[0,0]
        values = [first_column, second_column, cos_sim1]
        zipped = zip(columns, values)
        a_dictionary = dict(zipped)
        print(a_dictionary)
        sim_list.append(a_dictionary)

  df1 = df1.append(sim_list)
  df2 = df1.drop_duplicates(keep='last')
  return(df2)

book_subset_NA['tokenized_sents'] = book_subset_NA.apply(lambda row: nltk.word_tokenize(row['book_desc']), axis=1)

input_sent = list(book_subset_NA["tokenized_sents"])
input_sent

sentences = client.encode(input_sent, show_tokens=True, is_tokenized=True)

books_comparison = synopsis_similarity(book_subset_NA, sentences)

book_comparison.shape

books_comparison.to_csv("synopsis_similarity.csv")
files.download( "synopsis_similarity.csv" )